# Logstash Pipeline Configuration for RFID Asset Tracking System
# Version: 1.0.0
# Dependencies:
# - logstash-input-beats v7.17.0
# - logstash-filter-json v3.2.0
# - logstash-filter-grok v4.4.0
# - logstash-output-elasticsearch v11.0.0

input {
  # Filebeat input for collecting logs from microservices
  beats {
    port => 5044
    host => "0.0.0.0"
    client_inactivity_timeout => 60
    ssl_enabled => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_verify_mode => "force_peer"
  }

  # TCP input for direct application logging
  tcp {
    port => 5000
    codec => json
    ssl_enable => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_verify => true
  }
}

# Custom RFID-specific Grok patterns
patterns_dir => ["/etc/logstash/patterns"]

filter {
  # Parse JSON formatted logs
  json {
    source => "message"
    target => "parsed_json"
    remove_field => ["message"]
  }

  # Apply custom RFID patterns and general log patterns
  grok {
    match => {
      "message" => [
        "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} %{GREEDYDATA:log_message}",
        "%{TIMESTAMP_ISO8601:timestamp} %{WORD:reader_id} %{WORD:tag_id} %{NUMBER:signal_strength:float} %{WORD:read_status}",
        "%{TIMESTAMP_ISO8601:timestamp} %{WORD:event_type} Asset:%{WORD:asset_id} Location:%{WORD:location_id} Status:%{WORD:status}"
      ]
    }
    tag_on_failure => ["_grokparsefailure"]
  }

  # Add Kubernetes metadata
  mutate {
    add_field => {
      "environment" => "%{[kubernetes][namespace]}"
      "service" => "%{[kubernetes][container][name]}"
      "data_center" => "%{[kubernetes][node][name]}"
    }
  }

  # Timestamp processing
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
    remove_field => ["timestamp"]
  }

  # Enrich RFID-specific events
  if [reader_id] {
    mutate {
      add_field => {
        "event_type" => "rfid_read"
        "source_type" => "reader"
      }
    }
  }

  # Security event tagging
  if [log_level] in ["ERROR", "WARN"] {
    mutate {
      add_tag => ["security_alert"]
    }
  }
}

output {
  # Primary Elasticsearch output
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS}"]
    user => "${ELASTIC_USERNAME}"
    password => "${ELASTIC_PASSWORD}"
    ssl_enabled => true
    ssl_certificate_verification => true
    cacert => "/etc/logstash/certs/ca.crt"
    
    # Index management
    index => "rfid-tracking-logs-%{+YYYY.MM.dd}"
    template_name => "rfid-tracking"
    template_overwrite => true
    
    # Performance settings
    bulk_size => 2000
    flush_size => 1000
    idle_flush_time => 5
  }

  # Security events to dedicated index
  if "security_alert" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS}"]
      user => "${ELASTIC_USERNAME}"
      password => "${ELASTIC_PASSWORD}"
      ssl_enabled => true
      ssl_certificate_verification => true
      cacert => "/etc/logstash/certs/ca.crt"
      index => "rfid-security-alerts-%{+YYYY.MM.dd}"
    }
  }
}

# Performance tuning
pipeline.workers: 4
pipeline.batch.size: 2000
pipeline.batch.delay: 50
queue.type: persisted
queue.max_bytes: 1gb
queue.checkpoint.writes: 1000

# Monitoring configuration
xpack.monitoring.enabled: true
xpack.monitoring.elasticsearch.hosts: ["http://elasticsearch:9200"]
xpack.monitoring.elasticsearch.username: "${ELASTIC_USERNAME}"
xpack.monitoring.elasticsearch.password: "${ELASTIC_PASSWORD}"
xpack.monitoring.elasticsearch.ssl.enabled: true
xpack.monitoring.elasticsearch.ssl.verification_mode: certificate
xpack.monitoring.collection.interval: 10s